{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567e613e-dbf8-4904-905c-101f2459e8a3",
   "metadata": {},
   "source": [
    "# R² Improved from 0.952 to 0.999 — Benchmarking ML Regressors\n",
    "\n",
    "In this notebook, we aim to improve the prediction performance of a regression model on a credit dataset.  \n",
    "We start with a simple Linear Regression model and then try various machine learning techniques  \n",
    "to boost the R² score from **0.952** to **0.999**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b55c35-a27f-4fd2-8cf8-b019a2d2eb8e",
   "metadata": {},
   "source": [
    "# Step 1: Train a Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c3e23-a951-4483-918f-411f7f6a0a47",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We read the dataset using pandas and removed the \"ID\" column because it is not useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04551185-9e17-4d97-aa22-947f43fb2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import optuna\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.distributions import FloatDistribution, IntDistribution\n",
    "from tabpfn import TabPFNRegressor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "df = pd.read_csv(\"Credit_Data.csv\")\n",
    "df = df.drop(columns=[\"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2a2c5-e99f-4f21-84c0-b35c094d64ac",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f373ad4-4fb2-4dc2-937e-2892332dac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Student_Yes</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>12.096</td>\n",
       "      <td>4100</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>13.364</td>\n",
       "      <td>3838</td>\n",
       "      <td>296</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>57.872</td>\n",
       "      <td>4171</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>37.728</td>\n",
       "      <td>2525</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>18.701</td>\n",
       "      <td>5524</td>\n",
       "      <td>415</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Income  Limit  Rating  Cards  Age  Education  Balance  Gender_Male  \\\n",
       "0     14.891   3606     283      2   34         11      333          1.0   \n",
       "1    106.025   6645     483      3   82         15      903          0.0   \n",
       "2    104.593   7075     514      4   71         11      580          1.0   \n",
       "3    148.924   9504     681      3   36         11      964          0.0   \n",
       "4     55.882   4897     357      2   68         16      331          1.0   \n",
       "..       ...    ...     ...    ...  ...        ...      ...          ...   \n",
       "395   12.096   4100     307      3   32         13      560          1.0   \n",
       "396   13.364   3838     296      5   65         17      480          1.0   \n",
       "397   57.872   4171     321      5   67         12      138          0.0   \n",
       "398   37.728   2525     192      1   44         13        0          1.0   \n",
       "399   18.701   5524     415      5   64          7      966          0.0   \n",
       "\n",
       "     Student_Yes  Married_Yes  Ethnicity_Asian  Ethnicity_Caucasian  \n",
       "0            0.0          1.0              0.0                  1.0  \n",
       "1            1.0          1.0              1.0                  0.0  \n",
       "2            0.0          0.0              1.0                  0.0  \n",
       "3            0.0          0.0              1.0                  0.0  \n",
       "4            0.0          1.0              0.0                  1.0  \n",
       "..           ...          ...              ...                  ...  \n",
       "395          0.0          1.0              0.0                  1.0  \n",
       "396          0.0          0.0              0.0                  0.0  \n",
       "397          0.0          1.0              0.0                  1.0  \n",
       "398          0.0          1.0              0.0                  1.0  \n",
       "399          0.0          0.0              1.0                  0.0  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = [x for x in df.columns if df[x].dtypes in [\"int\", \"float\"]]\n",
    "cat_cols = [x for x in df.columns if df[x].dtypes not in [\"int\", \"float\"]]\n",
    "\n",
    "dummies = pd.get_dummies(df[cat_cols], drop_first=True).astype(float)\n",
    "df = pd.concat([df[num_cols], dummies], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc642064-37e2-41c3-ab87-7d9c0a9867a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"Balance\"\n",
    "X = df.drop(columns = [target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa17fea-da4e-48f4-821a-e0ab43b8675f",
   "metadata": {},
   "source": [
    "## Train a Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fcac76-f4c7-470c-9c71-f8f1e00db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522674050276462\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train,y_train)\n",
    "y_pred = linear_model.predict(X_test)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece433b-8ae0-4704-b578-bbc3de36b913",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "This result is quite strong, but we aim to improve it further.  \n",
    "To do this, we will try different machine learning models and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207032f9-a6c5-4019-8d70-29a48ccc86ce",
   "metadata": {},
   "source": [
    "# Step 2: Trying different ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f97b6-5a24-4bee-972b-a4c602f8e097",
   "metadata": {},
   "source": [
    "## GridSearchCV with Multiple Models\n",
    "\n",
    "We used `GridSearchCV` to tune hyperparameters of several regressors:\n",
    "\n",
    "- Random Forest\n",
    "- Ridge\n",
    "- Lasso\n",
    "- Decision Tree\n",
    "\n",
    "Each model was evaluated using **10-fold cross-validation** with **R² score** as the metric.\n",
    "\n",
    "Below are the best scores obtained for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fc87f2-993f-4f77-b0c6-d533d108715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Best Params: {'regressor__max_depth': 10, 'regressor__n_estimators': 100}\n",
      "RandomForest Best R2: 0.9512\n",
      "Ridge Best Params: {'regressor__alpha': 1}\n",
      "Ridge Best R2: 0.9500\n",
      "Lasso Best Params: {'regressor__alpha': 0.01}\n",
      "Lasso Best R2: 0.9500\n",
      "DecisionTree Best Params: {'regressor__max_depth': None, 'regressor__min_samples_split': 10}\n",
      "DecisionTree Best R2: 0.9146\n"
     ]
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42),\n",
    "        \"params\": {\n",
    "            \"regressor__n_estimators\": [10,50,100],\n",
    "            \"regressor__max_depth\": [3,5,10]\n",
    "        }\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"model\": Ridge(),\n",
    "        \"params\": {\n",
    "            \"regressor__alpha\": [0.01,0.1,1,10]\n",
    "        }\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\n",
    "            \"regressor__alpha\": [0.01,0.1,1,10]\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor(random_state=42),\n",
    "        \"params\": {\n",
    "            \"regressor__max_depth\": [3,5,7, None],\n",
    "            \"regressor__min_samples_split\": [2,5,10]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", config[\"model\"])\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, config[\"params\"], cv=10, scoring = \"r2\", n_jobs=-1)\n",
    "    grid.fit(X,y)\n",
    "\n",
    "    print(f\"{name} Best Params: {grid.best_params_}\")\n",
    "    print(f\"{name} Best R2: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb58b9-f4b9-4bcb-a716-e068971ed40f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Optuna\n",
    "\n",
    "We used **OptunaSearchCV** to optimize the following regressors:\n",
    "\n",
    "- Ridge\n",
    "- Lasso\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n",
    "Each model was tuned with **300 trials** using **5-fold cross-validation**, and **R² score** was used as the evaluation metric.\n",
    "\n",
    "Below are the best hyperparameters and corresponding scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d49f61c-2826-46bc-a756-b550a0d921f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Ridge...\n",
      "Ridge Best Params: {'regressor__alpha': 0.27170517842424147}\n",
      "Ridge Best R2: 0.9513\n",
      "\n",
      "Optimizing Lasso...\n",
      "Lasso Best Params: {'regressor__alpha': 0.00010000680239437019}\n",
      "Lasso Best R2: 0.9512\n",
      "\n",
      "Optimizing DecisionTree...\n",
      "DecisionTree Best Params: {'regressor__max_depth': 16, 'regressor__min_samples_split': 9}\n",
      "DecisionTree Best R2: 0.9136\n",
      "\n",
      "Optimizing RandomForest...\n",
      "RandomForest Best Params: {'regressor__n_estimators': 53, 'regressor__max_depth': 11}\n",
      "RandomForest Best R2: 0.9397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Ridge\": {\n",
    "        \"model\": Ridge(),\n",
    "        \"params\": {\n",
    "            \"regressor__alpha\": FloatDistribution(1e-4, 1e2, log=True)\n",
    "        }\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\n",
    "            \"regressor__alpha\": FloatDistribution(1e-4, 1e2, log=True)\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor(random_state=42),\n",
    "        \"params\": {\n",
    "            \"regressor__max_depth\": IntDistribution(2, 20),\n",
    "            \"regressor__min_samples_split\": IntDistribution(2, 20)\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42),\n",
    "        \"params\": {\n",
    "            \"regressor__n_estimators\": IntDistribution(10, 200),\n",
    "            \"regressor__max_depth\": IntDistribution(2, 20)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", config[\"model\"])\n",
    "    ])\n",
    "\n",
    "    optuna_search = OptunaSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=config[\"params\"],\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring=\"r2\",\n",
    "        n_trials=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    optuna_search.fit(X, y)\n",
    "\n",
    "    print(f\"{name} Best Params: {optuna_search.best_params_}\")\n",
    "    print(f\"{name} Best R2: {optuna_search.best_score_:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ab3f2-66bd-4033-91f9-b28633f28d61",
   "metadata": {},
   "source": [
    "## AutoML with AutoGluon\n",
    "\n",
    "We used **AutoGluon TabularPredictor** to automatically train and tune regression models.\n",
    "\n",
    "- Training data was split into 80% train and 20% test.\n",
    "- Models used: **Linear Regression (LR)** and **Random Forest (RF)**.\n",
    "- Preset used: `\"best_quality\"` for best possible performance.\n",
    "- Evaluation metric: **R² score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28aea871-a612-4bac-93e3-1dc7ca5ab61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train = train_df.drop(columns=\"Balance\")\n",
    "y_train = train_df[\"Balance\"]\n",
    "X_test = test_df.drop(columns=\"Balance\")\n",
    "y_test = test_df[\"Balance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0793f7-9a64-4f17-ba88-5ee60adf26c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250612_072647\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:26 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       6.91 GB / 16.00 GB (43.2%)\n",
      "Disk Space Avail:   40.05 GB / 228.27 GB (17.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-06-12 10:26:47,283\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-06-12 10:26:48,213\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/Users/deniz/Codes/Portfolio/AutogluonModels/ag-20250612_072647/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Beginning AutoGluon training ... Time limit = 898s\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m AutoGluon will save models to \"/Users/deniz/Codes/Portfolio/AutogluonModels/ag-20250612_072647/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Train Data Rows:    284\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Train Data Columns: 11\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Label Column:       Balance\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tAvailable Memory:                    6691.73 MB\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t('float', []) : 6 | ['Income', 'Gender_Male', 'Student_Yes', 'Married_Yes', 'Ethnicity_Asian', ...]\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t('int', [])   : 5 | ['Limit', 'Rating', 'Cards', 'Age', 'Education']\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t('float', [])     : 1 | ['Income']\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t('int', [])       : 5 | ['Limit', 'Rating', 'Cards', 'Age', 'Education']\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t\t('int', ['bool']) : 5 | ['Gender_Male', 'Student_Yes', 'Married_Yes', 'Ethnicity_Asian', 'Ethnicity_Caucasian']\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t11 features in original data used to generate 11 features in processed data.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Data preprocessing and feature engineering runtime = 0.01s ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t'LR': [{}],\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t'RF': [{}],\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: RandomForest_BAG_L1 ... Training model for up to 598.72s of the 898.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m /opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.9301\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: LinearModel_BAG_L1 ... Training model for up to 597.82s of the 897.40s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
      "\u001b[36m(_ray_fit pid=2263)\u001b[0m /opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (248). n_quantiles is set to n_samples.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.8946\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 895.06s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tEnsemble Weights: {'RandomForest_BAG_L1': 0.7, 'LinearModel_BAG_L1': 0.3}\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.9381\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: RandomForest_BAG_L2 ... Training model for up to 895.05s of the 895.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m /opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.9574\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: LinearModel_BAG_L2 ... Training model for up to 894.75s of the 894.75s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
      "\u001b[36m(_ray_fit pid=2283)\u001b[0m   warnings.warn(\u001b[32m [repeated 10x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.9449\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 892.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \tEnsemble Weights: {'RandomForest_BAG_L2': 0.846, 'LinearModel_BAG_L2': 0.154}\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.9578\t = Validation score   (r2)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m AutoGluon training complete, total runtime = 6.03s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 725.7 rows/s (36 batch size)\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/deniz/Codes/Portfolio/AutogluonModels/ag-20250612_072647/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=2253)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.988949   0.957843          r2        0.168401       0.122818  0.947834                 0.000662                0.000258           0.007810            3       True          6\n",
      "1  RandomForest_BAG_L2       0.988216   0.957387          r2        0.143340       0.102117  0.800778                 0.059070                0.041643           0.246414            2       True          4\n",
      "2   LinearModel_BAG_L2       0.975713   0.944936          r2        0.108669       0.080917  0.693610                 0.024399                0.020443           0.139246            2       True          5\n",
      "3  WeightedEnsemble_L2       0.971418   0.938141          r2        0.085003       0.060711  0.562264                 0.000733                0.000237           0.007900            2       True          3\n",
      "4  RandomForest_BAG_L1       0.964109   0.930107          r2        0.059993       0.041938  0.256410                 0.059993                0.041938           0.256410            1       True          1\n",
      "5   LinearModel_BAG_L1       0.944767   0.894600          r2        0.024277       0.018536  0.297954                 0.024277                0.018536           0.297954            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t9s\t = DyStack   runtime |\t3591s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 3591s\n",
      "AutoGluon will save models to \"/Users/deniz/Codes/Portfolio/AutogluonModels/ag-20250612_072647\"\n",
      "Train Data Rows:    320\n",
      "Train Data Columns: 11\n",
      "Label Column:       Balance\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6451.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['Income', 'Gender_Male', 'Student_Yes', 'Married_Yes', 'Ethnicity_Asian', ...]\n",
      "\t\t('int', [])   : 5 | ['Limit', 'Rating', 'Cards', 'Age', 'Education']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 1 | ['Income']\n",
      "\t\t('int', [])       : 5 | ['Limit', 'Rating', 'Cards', 'Age', 'Education']\n",
      "\t\t('int', ['bool']) : 5 | ['Gender_Male', 'Student_Yes', 'Married_Yes', 'Ethnicity_Asian', 'Ethnicity_Caucasian']\n",
      "\t0.0s = Fit runtime\n",
      "\t11 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'LR': [{}],\n",
      "\t'RF': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 2393.16s of the 3590.64s of remaining time.\n",
      "\t0.9417\t = Validation score   (r2)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LinearModel_BAG_L1 ... Training model for up to 2392.89s of the 3590.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
      "\t0.9003\t = Validation score   (r2)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3587.95s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForest_BAG_L1': 0.727, 'LinearModel_BAG_L1': 0.273}\n",
      "\t0.9482\t = Validation score   (r2)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 3587.93s of the 3587.93s of remaining time.\n",
      "\t0.9639\t = Validation score   (r2)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LinearModel_BAG_L2 ... Training model for up to 3587.45s of the 3587.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
      "\t0.9573\t = Validation score   (r2)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 3585.16s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForest_BAG_L2': 0.778, 'LinearModel_BAG_L2': 0.222}\n",
      "\t0.9645\t = Validation score   (r2)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.56s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 738.5 rows/s (40 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/deniz/Codes/Portfolio/AutogluonModels/ag-20250612_072647\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=\"Balance\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"r2\"\n",
    ").fit(\n",
    "    train_data=train_df,\n",
    "    presets=\"best_quality\",\n",
    "    hyperparameters={\"LR\": {}, \"RF\": {}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b8e486-08f9-4bf7-95ee-003b6ccceb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score: 0.9696\n"
     ]
    }
   ],
   "source": [
    "y_test = test_df[\"Balance\"]\n",
    "X_test = test_df.drop(columns=[\"Balance\"])\n",
    "\n",
    "y_pred = predictor.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R² score:\", round(r2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ae240-a489-447d-9fbe-f4465b180fe7",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) Regressor\n",
    "\n",
    "We trained a **Neural Network Regressor (MLPRegressor)** using the following configuration:\n",
    "\n",
    "- Two hidden layers: 100 and 50 neurons\n",
    "- Activation function: ReLU\n",
    "- Solver: Adam\n",
    "- Max iterations: 1000\n",
    "- Data was scaled using `StandardScaler`\n",
    "\n",
    "This approach yielded a very high R² score, but we also checked for overfitting by comparing train and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b86995-6429-410a-a1ed-b5c38ec3961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² (Train): 0.9982149180585883\n",
      "R² (Test): 0.9905390319299068\n"
     ]
    }
   ],
   "source": [
    "target_variable = \"Balance\"\n",
    "X = df.drop(columns = [target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                         max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "print(\"R² (Train):\", r2_score(y_train, y_train_pred))\n",
    "print(\"R² (Test):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30b66f-dc13-45a9-8dd7-9e5fbc2d29ad",
   "metadata": {},
   "source": [
    "#### Checking Overfitting for Multilayer Perceptron (MLP) Regressor\n",
    "\n",
    "To check if the model is overfitting, we use **cross-validation**.  \n",
    "We train and test the model on different parts of the data.  \n",
    "If the scores are very different, the model might be overfitting.\n",
    "\n",
    "Below are the R² scores from 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33930138-483b-4bfa-9d70-fe448b447990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated R² scores: [0.99579611 0.99588718 0.99443395 0.99219605 0.99285242]\n",
      "Mean R²: 0.9942331419814557\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"Cross-validated R² scores:\", scores)\n",
    "print(\"Mean R²:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14edbf-db72-40da-a282-35b3e7ba7625",
   "metadata": {},
   "source": [
    "## TabPFN Regressor\n",
    "\n",
    "We use the TabPFN model for regression.  \n",
    "It is a very powerful pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7be338a-e7ad-4b1a-8b4f-a2e61d1f034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score: 0.9988414645195007\n"
     ]
    }
   ],
   "source": [
    "reg = TabPFNRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"R² score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb117b4-af7d-4908-8db7-b2cd2f0be015",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Step 1: we started with a baseline Linear Regression model and achieved an R² score of **0.9522**.  \n",
    "\n",
    "Step 2: By experimenting with multiple algorithms — including Ridge, Lasso, Random Forest, MLP, and TabPFN —  \n",
    "we significantly improved model performance. The best result came from **TabPFN**,  \n",
    "reaching an impressive R² score of **0.999**.\n",
    "\n",
    "This shows how testing diverse models can greatly improve predictive accuracy in regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dff2b-1db1-4507-822c-96f22846ea8c",
   "metadata": {},
   "source": [
    "# Conclusion: Comparing R² Scores\n",
    "\n",
    "We started with a simple Linear Regression model that gave an R² score of **0.952**.  \n",
    "To improve this, we tried various models and optimization strategies:\n",
    "\n",
    "| Model                         | R² Score |\n",
    "|------------------------------|----------|\n",
    "| **Linear Regression**        | 0.952    |\n",
    "| GridSearch - RandomForest    | 0.951    |\n",
    "| GridSearch - Ridge           | 0.950    |\n",
    "| GridSearch - Lasso           | 0.950    |\n",
    "| GridSearch - DecisionTree    | 0.915    |\n",
    "| Optuna - Ridge               | 0.951    |\n",
    "| Optuna - Lasso               | 0.951    |\n",
    "| Optuna - DecisionTree        | 0.914    |\n",
    "| Optuna - RandomForest        | 0.940    |\n",
    "| AutoGluon                    | 0.970    |\n",
    "| MLP (Neural Network)         | 0.991    |\n",
    "| **TabPFN Regressor**         | 0.999    |\n",
    "\n",
    "The **TabPFN Regressor** achieved the best performance with an R² of **0.999**, showing significant improvement over the baseline. \n",
    "\n",
    "This highlights the power of modern neural architectures and AutoML tools for regression problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
