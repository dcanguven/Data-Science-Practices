{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9306744-dd8e-4a45-afa3-250fe0bcf22d",
   "metadata": {},
   "source": [
    "# PREDICTING SCRAP RATIO: A DATA-DRIVEN APPROACH TO IMPROVE PRODUCTION PLANNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f171c3f-6668-40eb-bbe5-a43ebf741394",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook presents a comprehensive data science project aimed at predicting the scrap ratio using **synthetic production data**. The target is to provide better production planning by predicting the scrap ratio before production starts, which can help in reducing waste and improving efficiency.\n",
    "\n",
    "### Key Highlights:\n",
    "\n",
    "* **Feature Engineering**: This notebook demonstrates several feature engineering techniques that enhance model performance. It includes custom functions to make the process more efficient and maintain tidy, readable code.\n",
    "* **Encoding Methods**: The notebook applies One-Hot Encoding and Frequency Encoding to handle categorical variables, showcasing their differences and use cases.\n",
    "* **Machine Learning Models**:\n",
    "    * Random Forest Regressor: A tree-based ensemble model that performs well on tabular data.\n",
    "    * LightGBM Regressor: A gradient boosting model known for its speed and efficiency.\n",
    "\n",
    "### Goal:\n",
    "\n",
    "The goal of this project is to predict the scrap ratio based on planning data, allowing production planners to anticipate waste and optimize the production process. By improving the prediction of scrap ratio, better production planning decisions can be made, ultimately leading to reduced costs and improved product quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e303831-4f0a-4923-8fca-fd4e0d364b3c",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d528595-fdbf-4d1f-9aa1-a428294f5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV, cross_val_predict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99feaf3a-4ce7-4287-826f-545e994145e7",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a757233-7831-4fb7-bff1-a863f8a7276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 125 ms, total: 13.2 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_main = pd.read_excel(\"production_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c9b82d-f3a3-4092-b035-0b979c1cd0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: \n",
      "Rows: 119021 \n",
      "Columns: 24\n"
     ]
    }
   ],
   "source": [
    "df = df_main.copy()\n",
    "print(\"Shape of dataframe: \\nRows:\", df.shape[0], \"\\nColumns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c75c40-e50a-4a47-b28e-b91d4ce506bf",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55be632a-6508-4b83-b447-15ad89779d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify and translate column names\n",
    "# Drop unnecessary columns\n",
    "\n",
    "def column_name_preprocess(df):\n",
    "    df = df.rename(columns = {'Sipariş': \"order_no\", 'Tanım': \"shorttext\", 'Tanım.1':\"recycle_info\", 'Tanım.2': \"color\", \n",
    "                              'Baz': \"base_code\", 'RomaGofraj':\"embossing\", 'GLOSS': \"gloss\", 'Renk': \"top_color\",\n",
    "                              'Baskı':\"printed\", 'WE-Datum': \"date\", 'Çekme mkt.': \"raw_kg\",'Scrap': \"raw_scrap_kg\",\n",
    "                              'Çekme mkt..1': \"color_kg\", 'Scrap.1':\"color_scrap_kg\", 'Kalınlık': \"thickness\"})\n",
    "    df = df.drop(['İşyeri','Teyit trh.', 'Birim', 'TÖB', 'TÖB.1', \n",
    "                  'TÖB.2', 'İht.mkt.', 'İhtiyaç mkt.', 'Hdf.miktar'],axis=1)\n",
    "    return df\n",
    "\n",
    "# Add necessary features\n",
    "# Drop unnecessary ones\n",
    "\n",
    "def feature_preprocess(df):\n",
    "    df[\"material_type\"] = df[\"shorttext\"].apply(lambda x: \"PVC\" if \"PER\" in x else \"ABS\")\n",
    "    df[\"jumbo\"] = df[\"shorttext\"].apply(lambda x: \"jr\" if \"0001\" in x else \"slitted\")\n",
    "    df[\"recycle\"] = df[\"recycle_info\"].apply(lambda x: \"Recycled\" if \"Geri\" in x else \"Not_recycled\")\n",
    "    df[\"total_kg\"] = df[\"raw_kg\"] + df[\"raw_scrap_kg\"]\n",
    "    df[\"total_scrap_kg\"] = df[\"raw_scrap_kg\"] + df[\"color_scrap_kg\"]\n",
    "    df[\"scrap_ratio\"] = df[\"total_scrap_kg\"] / df[\"total_kg\"]\n",
    "    df = df.drop([\"recycle_info\", \"shorttext\", \"raw_kg\", \"raw_scrap_kg\", 'order_no', 'color', 'top_color',\n",
    "                 'color_kg', 'color_scrap_kg'], axis=1)\n",
    "    df = df[df['thickness'] != 9.99]\n",
    "    df.printed = df.printed/2\n",
    "    df=df[df[\"total_kg\"]>100]\n",
    "    return df\n",
    "\n",
    "# Handle missing colornames in data\n",
    "\n",
    "def replace_invalid_colors_optimized(df, invalid_colors):\n",
    "    valid_colors = df.loc[~df['colorname'].isin(invalid_colors) & df['colorname'].notna()]\n",
    "    valid_colors = valid_colors.groupby('base_code')['colorname'].first().reset_index()\n",
    "\n",
    "    df = df.merge(valid_colors, on='base_code', how='left', suffixes=('', '_valid'))\n",
    "\n",
    "    df['colorname'] = df.apply(\n",
    "        lambda row: row['colorname_valid'] if (pd.isnull(row['colorname']) or row['colorname'] in invalid_colors) \n",
    "        else row['colorname'], axis=1\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['colorname_valid'])\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "def missing_values(df):\n",
    "    \n",
    "    invalid_colors = ['VN800673', 'A-M-T', '25kg', 'PVC', 'SİYAHI', 'R-FK-3', '3RLP', \n",
    "                      'İşlevsiz', 'ABS', \"\", \"NA\", \"krem\"]\n",
    "    \n",
    "    df = df.dropna(subset=[\"recycle_info\"])\n",
    "    df[\"color\"] = df[\"color\"].str.replace('*', '', regex=False)\n",
    "    df[\"color\"] = df[\"color\"].str.rstrip()\n",
    "    df[\"colorname\"] = df[\"color\"].apply(lambda x: x.split(\" \")[-1] if pd.notna(x) else \"NA\")\n",
    "    df = replace_invalid_colors_optimized(df, invalid_colors)\n",
    "    df['printed'] = df.apply(lambda row: 0 if row['base_code'] == row['top_color'] else 2.0, axis=1)\n",
    "    df = df.fillna(\"NotAvailable\")\n",
    "    return df\n",
    "\n",
    "# scrap_ratio adjustment\n",
    "\n",
    "def adjust_scrap_values(df):\n",
    "    df.loc[df['scrap_ratio'] > 1, 'total_scrap_kg'] = df['total_kg']\n",
    "    df.loc[df['scrap_ratio'] > 1, 'scrap_ratio'] = 1\n",
    "    df.loc[df['scrap_ratio'] < 0, 'total_scrap_kg'] = 0\n",
    "    df.loc[df['scrap_ratio'] < 0, 'scrap_ratio'] = 0\n",
    "    return df\n",
    "\n",
    "# group base codes\n",
    "\n",
    "def categorize_base_code(df):\n",
    "    frequency_counts = df['base_code'].value_counts()\n",
    "\n",
    "    def categorize(freq, base_code):\n",
    "        if freq < 10:\n",
    "            return 'E'\n",
    "        elif 10 <= freq < 50:\n",
    "            return 'D'\n",
    "        elif 50 <= freq < 100:\n",
    "            return 'C'\n",
    "        elif 100 <= freq < 150:\n",
    "            return 'B'\n",
    "        elif 150 <= freq < 200:\n",
    "            return 'A'\n",
    "        else:\n",
    "            return base_code\n",
    "\n",
    "    df['base_code'] = df['base_code'].map(frequency_counts).combine(df['base_code'], categorize)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f6e409-713b-43df-85ad-e23e2b37c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 24.7 ms, total: 1.41 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = column_name_preprocess(df)\n",
    "df = missing_values(df)\n",
    "df = feature_preprocess(df)\n",
    "df = adjust_scrap_values(df)\n",
    "df = categorize_base_code(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6297660-7002-4ef4-ae79-417ab8ec2c67",
   "metadata": {},
   "source": [
    "# 4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc0b960-3828-4e0e-8565-36645b8a159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoded = df['base_code'].value_counts(normalize=True)\n",
    "df['base_code_encoded'] = df['base_code'].map(frequency_encoded)\n",
    "\n",
    "frequency_encoded = df['embossing'].value_counts(normalize=True)\n",
    "df['embossing_encoded'] = df['embossing'].map(frequency_encoded)\n",
    "\n",
    "frequency_encoded = df['gloss'].value_counts(normalize=True)\n",
    "df['gloss_encoded'] = df['gloss'].map(frequency_encoded)\n",
    "\n",
    "frequency_encoded = df['colorname'].value_counts(normalize=True)\n",
    "df['colorname_encoded'] = df['colorname'].map(frequency_encoded)\n",
    "\n",
    "df = df.drop([\"base_code\", \"embossing\", \"gloss\", \"colorname\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44fac804-a9b4-4d90-a1ee-fb494c750c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[\"number\"])\n",
    "cat_cols = df.select_dtypes(include= [\"object\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869ea298-e9fa-4737-a9e8-4f16a934c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_one_hot_encoded = df[[\"material_type\", \"recycle\", \"jumbo\"]]\n",
    "dummies = pd.get_dummies(to_be_one_hot_encoded, drop_first=True).astype(\"int8\")\n",
    "y = df.scrap_ratio\n",
    "X_ = num_cols.drop([\"scrap_ratio\", \"total_scrap_kg\"], axis=1)\n",
    "X = pd.concat([X_, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28cef05-39db-4eba-bf17-1d5884f13948",
   "metadata": {},
   "source": [
    "# 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d901a425-17fc-4050-9e1b-e1d244afe128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best Parameters:  {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': True}\n",
      "Cross-validation MSE: 0.008868508138510778\n",
      "Test MSE: 0.008731706735990774\n",
      "CPU times: user 1min 26s, sys: 848 ms, total: 1min 26s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform Random Forest Regressor\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [150],             # [100, 150],\n",
    "    'max_depth': [None],               # [10, None]\n",
    "    'min_samples_split': [2],          # [2, 5]\n",
    "    'min_samples_leaf': [1],           # [1, 2]\n",
    "    'bootstrap': [True]                # [True]  \n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=kfold, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error', \n",
    "                                   random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "cv_scores = cross_val_score(random_search.best_estimator_, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validation MSE: {-cv_scores.mean()}\")\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b51e10ab-6159-49aa-8808-8c12c9aeffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 478\n",
      "[LightGBM] [Info] Number of data points in the train set: 93566, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.203043\n",
      "Best Parameters:  {'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 150, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 476\n",
      "[LightGBM] [Info] Number of data points in the train set: 62377, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.202943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 474\n",
      "[LightGBM] [Info] Number of data points in the train set: 62377, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.203097\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 477\n",
      "[LightGBM] [Info] Number of data points in the train set: 62378, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.203088\n",
      "Cross-validation MSE: 0.008499762014187048\n",
      "Test MSE: 0.008461444613473378\n",
      "CPU times: user 4.24 s, sys: 6.16 s, total: 10.4 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform LightGBM\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': [31, 50, 70],       \n",
    "    'max_depth': [-1, 10, 20],             \n",
    "    'learning_rate': [0.01, 0.1, 0.2], \n",
    "    'n_estimators': [100, 150],    \n",
    "    'min_child_samples': [20, 50],   \n",
    "    'subsample': [0.8, 1.0],   \n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=lgb_model, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=kfold, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error', \n",
    "                                   random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "cv_scores = cross_val_score(random_search.best_estimator_, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validation MSE: {-cv_scores.mean()}\")\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80c325-733f-4203-a8e0-82161698ed3a",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234b622-6dea-443a-b617-282b79bc2761",
   "metadata": {},
   "source": [
    "In this notebook, we explored various feature engineering techniques and applied both Random Forest Regressor and LightGBM Regressor models to predict the scrap ratio from planning data. After tuning the models, LightGBM achieved the best performance with a Mean Squared Error (MSE) of 0.00846, demonstrating its effectiveness in predicting the scrap ratio.\n",
    "\n",
    "With this model ready to make predictions, it can now be integrated into the production planning process. By forecasting the scrap ratio before production, planners can make data-driven decisions, anticipate potential waste, and optimize resources, ultimately leading to a more efficient and cost-effective production process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a2364-309c-49de-bf92-df62f838e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
